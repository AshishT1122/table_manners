{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTwpDNBnorUT"
      },
      "source": [
        "# **Table Manners**: Improving Digestion of Tabular Data via Contextualization and Handling of Non-Uniform Tables\n",
        "| Title    | **Table Manners**: Improving Digestion of Tabular Data via Contextualization and Handling of Non-Uniform Tables   |\n",
        "|-----------|----------------|\n",
        "| Authors  | Ashish Thakur & Emily Okabe|\n",
        "| Mentor   | Heidi Zhang         |\n",
        "\n",
        "## ColBERTv2: Indexing & Search Notebook\n",
        "Based on [ColBERTv2 notebook](https://colab.research.google.com/github/stanford-futuredata/ColBERT/blob/main/docs/intro2new.ipynb), sourced from [ColBERTv2](https://github.com/stanford-futuredata/ColBERT).\n",
        "\n",
        "If you're working in Google Colab, we recommend selecting \"GPU\" as your hardware accelerator in the runtime settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrOqlYrd_ZZn"
      },
      "source": [
        "## **Preparing collections, queries, and answers**\n",
        "*First, we'll import the relevant classes. Note that `Indexer` and `Searcher` are the key actors here. Next, we'll download the necessary dependencies.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl_YBBPTo5AZ"
      },
      "outputs": [],
      "source": [
        "!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
        "import sys; sys.path.insert(0, 'ColBERT/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmBi2UT5pxb3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    !pip install -U pip\n",
        "    !pip install -e ColBERT/['faiss-gpu','torch']\n",
        "except Exception:\n",
        "  import sys; sys.path.insert(0, 'ColBERT/')\n",
        "  try:\n",
        "    from colbert import Indexer, Searcher\n",
        "  except Exception:\n",
        "    print(\"If you're running outside Colab, please make sure you install ColBERT in conda following the instructions in ColBERTv2's README. You can also install (as above) with pip but it may install slower or less stable faiss or torch dependencies. Conda is recommended.\")\n",
        "    assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0jxbVar4kln"
      },
      "outputs": [],
      "source": [
        "import colbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQg9A-dtp1nB"
      },
      "outputs": [],
      "source": [
        "from colbert import Indexer, Searcher\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert.data import Queries, Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF7lv8jvq-ut"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ci3N-RC-mPU"
      },
      "source": [
        "#### Downloading \"train\" split of CompMix Dataset\n",
        "*Queries are narrowed down to those pertaining to table data*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64ktecopaOBI"
      },
      "outputs": [],
      "source": [
        "queries_dataset = load_dataset(\"pchristm/CompMix\", split=\"train\")\n",
        "queries = [row['question'] for row in queries_dataset if row['answer_src'] == 'table']\n",
        "answers = [row['answer_text'] for row in queries_dataset if row['answer_src'] == 'table']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbFBm6hchJ5j"
      },
      "source": [
        "#### Downloading `wiki-all-8-4-tamber` variant of the `castorini/odqa-wiki-corpora` Dataset\n",
        "*Please ensure that you have sufficient disk space available before downloading wiki corpora dataset (recommend at least 50GB)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVjxl8mYUXNa"
      },
      "outputs": [],
      "source": [
        "collection_dataset = load_dataset(\"castorini/odqa-wiki-corpora\", \"wiki-all-8-4-tamber\")\n",
        "original_collection = [row['text'] for row in collection_dataset]\n",
        "\n",
        "f'Loaded {len(queries)} queries and {len(original_collection):,} passages'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FyN8AbS_RLI"
      },
      "source": [
        "### Preparing modified collection\n",
        "*Modified collection is prepared by replacing linearized table data from original collection with our enhanced lineareized table date (improved contextualization, handling of more non-uniform/non-standard tables, etc.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rZxl8xkfBaN"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "modified_collection = [row['text'] for row in collection_dataset if row['text'].count('|') <= 3]\n",
        "\n",
        "tsv_file_path = \"/content/linearized_table_data.tsv\"\n",
        "\n",
        "with open(tsv_file_path, 'r') as file:\n",
        "    tsv_reader = csv.DictReader(file, delimiter='\\t')\n",
        "    for row in tsv_reader:\n",
        "        original_collection.append(row['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJdAAbDu7PZ"
      },
      "source": [
        "## **Indexing**\n",
        "\n",
        "*For an efficient search, we can pre-compute the ColBERT representation of each set of passages and index each of them.*\n",
        "\n",
        "*Below, the `Indexer` takes a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from each index.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKAdVN5MvDKD"
      },
      "outputs": [],
      "source": [
        "nbits = 2   # encode each dimension with 2 bits\n",
        "doc_maxlen = 500 # truncate passages at 500 tokens\n",
        "\n",
        "original_dataset_index_name = f'wiki-all-8-4-tamber.original.{nbits}bits'\n",
        "modified_dataset_index_name = f'wiki-all-8-4-tamber.modified.{nbits}bits'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orKfQRmQv46u"
      },
      "source": [
        "*Now run the `Indexer` on each collection. This may take over 24 hours on a T4 GPU.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRiOnzxtwI0j"
      },
      "outputs": [],
      "source": [
        "checkpoint = 'colbert-ir/colbertv2.0'\n",
        "\n",
        "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use\n",
        "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
        "\n",
        "    original_indexer = Indexer(checkpoint=checkpoint, config=config)\n",
        "    original_indexer.index(name=original_dataset_index_name, collection=original_collection, overwrite=True)\n",
        "\n",
        "    modified_indexer = Indexer(checkpoint=checkpoint, config=config)\n",
        "    modified_indexer.index(name=modified_dataset_index_name, collection=modified_collection, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY6_D523yBFB"
      },
      "source": [
        "## **Searching and Evaluation**\n",
        "\n",
        "*Having built each of the indexes, we now prepare our searchers and evaluate the top-k retrieval accuracies across each collection.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epMBzLkFGBJs"
      },
      "outputs": [],
      "source": [
        "def is_valid_word(word: str):\n",
        "    \"\"\"Check if the word is valid (not a common/trivial word and not a single character unless a digit).\"\"\"\n",
        "    common_words = {\"an\", \"the\", \"or\", \"of\", \"and\"}\n",
        "    return word not in common_words and (len(word) > 1 or word.isdigit())\n",
        "\n",
        "def preprocess_answer(answer: str):\n",
        "    \"\"\"Preprocess the answer string by replacing punctuation with spaces and filtering out invalid words.\"\"\"\n",
        "    punctuation = \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
        "    answer_no_punct = answer.translate(str.maketrans(punctuation, ' ' * len(punctuation)))\n",
        "    return [word for word in answer_no_punct.split() if is_valid_word(word)]\n",
        "\n",
        "def check_passage_for_answer(passage: str, answer_words: list[str]):\n",
        "    \"\"\"Check if the passage contains at least one of the preprocessed answer words.\"\"\"\n",
        "    return any(word in passage for word in answer_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzBe6RdaGtVw"
      },
      "outputs": [],
      "source": [
        "def search_and_evaluate(searcher, queries, answers, top_k=10):\n",
        "    accuracy = {k: 0 for k in range(1, top_k+1)}\n",
        "    total_queries = len(queries)\n",
        "\n",
        "    for query, answer in zip(queries, answers):\n",
        "        results = searcher.search(query, k=top_k)\n",
        "        answer_words = preprocess_answer(answer)\n",
        "        found = False\n",
        "\n",
        "        for rank, (passage_id, _, _) in enumerate(zip(*results), start=1):\n",
        "            if check_passage_for_answer(searcher.collection[passage_id], answer_words):\n",
        "                for k in range(rank, top_k+1):\n",
        "                    accuracy[k] += 1\n",
        "                break\n",
        "\n",
        "    for k in range(1, top_k+1):\n",
        "        accuracy[k] /= total_queries\n",
        "        print(f\"Top-{k} Retrieval Accuracy: {accuracy[k]:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3x_FnVnyB0n"
      },
      "outputs": [],
      "source": [
        "with Run().context(RunConfig(experiment='notebook')):\n",
        "    original_searcher = Searcher(index=original_dataset_index_name, collection=original_collection)\n",
        "    modified_searcher = Searcher(index=modified_dataset_index_name, collection=modified_collection)\n",
        "\n",
        "    print(\"Evaluating Original Collection:\")\n",
        "    search_and_evaluate(original_searcher, queries, answers, top_k=10)\n",
        "\n",
        "    print(\"\\nEvaluating Modified Collection:\")\n",
        "    search_and_evaluate(modified_searcher, queries, answers, top_k=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
